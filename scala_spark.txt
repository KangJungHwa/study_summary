1. SPARK란?
Apache Spark는 인메모리 처리 기능을 갖춘 오픈소스 분산 컴퓨팅 프레임워크 입니다. Apache Spark는 최첨단 DAG 스케줄러, 쿼리 최적화 프로그램 및 실제 실행 엔진을 사용하여 배치 및 스트리밍 데이터 모두에 대해 높은 성능을 제공합니다.

Apache Spark를 사용하면 복잡한 데이터로부터 인사이트를 확보하는 알고리즘을 손쉽게 작성할 수 있습니다. Spark은 2014년에 최상위 아파치 프로젝트로 강화되었으며 지속적으로 확장되고 있습니다.

Spark은 병렬 응용 프로그램을 쉽게 만들 수있는 80 개 이상의 고급 연산자를 제공합니다. 또한 Scala, Python, R 및 SQL 쉘에서 대화식 으로 사용할 수 있습니다 .

스파크 능력을 포함하여 라이브러리의 스택 SQL 및 DataFrames , MLlib 기계 학습에 대한 GraphX , 및 스파크 스트리밍 . 동일한 응용 프로그램에서 이 라이브러리를 완벽하게 결합 할 수 있습니다.

Spark는 Hadoop, Apache Mesos, Kubernetes, 독립형 또는 클라우드에서 실행 되면. 다양한 형태(xml, json, text,csv …) 데이터 소스에 액세스 할 수 있습니다.

또한 대부분의 오픈소스 빅데이터 도구 와 통합되도록 설계 되었습니다. 뿐만 아니라 거의 대부분의 데이터 분석을 위한 상용 벤더들도 Spark를 지원하고 있습니다. Spark은 Hadoop 데이터 소스에 액세스 할 수 있는 것처럼 Hadoop 클러스터에서도 실행할 수 있습니다.




1.1 SPARK의 구성요소
 

1.1.1 SPARK Core
Spark Core는 거대한 데이터 세트의 병렬 처리 및 분산 처리의 기반역할을 하며Apache Spark Core의 핵심 기능은 다음과 같습니다.
1)	필수 I/O 기능을 담당합니다. 메모리 내 계산을 통해 MapReduce가 가지는 Disk IO의 단점을 극복합니다.
2)	작업 일정 관리 및 조정 외에도 스파크 내의 데이터인 RDD(Resilient Distributed Dataset)를 추출할 수 있는 기본적인 기능을 제공한다.

1.1.2 아파치 스파크 SQL
1.1.3. Apache Spark Streaming
1.1.4. Apache Spark MLlib
1.1.5. Apache Spark GraphX
1.1.6 Apache SparkR
1.	https://data-flair.training/blogs/spark-tutorial/
Have To wirte
맵리듀스와 비교
2. spark 개발에서 scala를 선택한 이유 현재 스칼라에서 막강한 기능인 DataSet을 파이썬에서 지원을 하고 있지 않다.
2. 용어 정리
2. spark 개발 환경구성
- spark 노트북
- spark-shell
- zeppelin
- scala IDE
- SPARK-REPL(read eval print loop)
대화형 스칼라 command line 인터프리터로 spark context가 자동 생성되어 sc(SparkContext)를 생성하는 과정이 필요하지 않다.
- SCALA에서 변수
스칼라에서 변수의 선언은 타입을 적어줄 필요가 없다.
val a=5
val b=”Hello World” 
- val VS var
  - val은 변경될 수 없는 변수를 선언 할 때 사용하고 var는 변경될 수 있는 변수를 
선언할 때 사용한다.
- SCALA collection : 배열
- 배열의 선언
val arr1= new Array[Int](10) // 빈 배열 선언방법
val arr2 = Array(1,3,4,5) // new 연산자와 자료형 없이도 선언이 가능하다.
val arr3 = Array(“a”, “b”, “c”)
val arr2 = Array("a", 1, 3.14) // 여러가지 자료형을 섞어서 쓸 수 있다.
val matrix = ofDim[Int](4,5)
- 배열의 조작
val arr2 = Array(1,3,4,5) 
        for(x <- arr2){
           println(x)
      }
- SCALA collection : List
리스트는 처음에 크기를 지정하지 않아도 됩니다. 리스트는 크기를 동적으로 늘렸다 줄였다 할 수 있습니다.
- 리스트의 선언
 val list=List() // 빈 리스트를 생성합니다. 리스트는 new를 이용하여 생성하지 않습니다.
 val list = List(“a”, “b”, “c”)
 - 리스트의 조작
    리스트에서 값을 가져오려면 index를 가지고 가져올 수 있습니다.
val list = List("a","b","c","d")
      for(i <- 0 to list.length-1  ){
        println(list(i))
      }
- object 함수
스칼라의 object는 자바의 static과 같이 instance를 생성하지 않고 사용할 수 있습니다.
스칼라의 object는 단하나의 인스턴스만 생성 가능한 싱글톤 객체입니다.
아래처럼 생성을 해놓으면 sc를 생성하는 함수인 getSparkContext를 인스턴스 생성없이 언제든지 생성할 수 있다.
object GetSparkContext {
  def getSparkContext(): SparkContext = {
    val conf = new SparkConf()
    conf.setMaster("local[*]").setAppName("PassingFunctionSample")
    new SparkContext(conf)
  }
}
아래와 같이 static 메소드를 호출하듯이 사용할 수 있다.
val sc= SContext.getSparkContext()
- Unit => java의 void 함수와 같다.
- RDD의 정의
   - 분산되어 존재하는 데이터 요소의 모임 스파크는 RDD에 있는 데이터들을 클러스터에 분산
저장 하여 클러스터 위에서 병렬 처리연산을 한다.
   - 분산되어 있는 변경이 불가능한 데이터 객체 모음이다. 
   - RDD가 속한 요소들은 파티션이라고 하는 더 작은 단위로 나눠질 수 있는데
이 파티션단위로 나눠졌서 병렬연산을 합니다.
   - 파티션은 연산과정에서 재구성되거나 네트웍을 통해 다른 서버로 이동하는 셔플링이 발생
하는데 셔플링은 전체작업 성능에 영향을 주므로 주의해서 다뤄야 합니다. 스파크에서는 셔플링이 발생할 수 잇는 주요 연산마다 파티션 갯수를 직접 지정할 수 있는 옵션을 제공합니다.
   - 스파크는 RDD는 생성과정을 기록해 뒀다가 원래 상태로 다시 복원하기 위한 기능을 가지고 
있습니다. 이때 rdd가 변경되는 것이 아니라 새로운 rdd가 생성된다.
   - 이처럼 rdd는 새로운 rdd가 생성될 수는 있어도 자신은 변경되지 않습니다.
   - resilient라는 단어가 이러한 복구능력을 말합니다.   
- RDD의 연산의 종료
- transfomation
- 존재하는 RDD를 기반으로 새로운 RDD를 생성한다.
- action
- RDD를 기초로 하여 연산된 결과값을 돌려준다.
- 리턴타입이 RDD가 아니면 action.
	   
- RDD의 생성
   - textFile (외부파일로 부터의 생성)
  	val lines=sc.textFile("/tmp/");
	val lines=sc.textFile("/tmp/*.txt");
        //SPARK에서는 gz 파일을 별다를 설정 없이 읽어 들일 수 있다. 
        val lines=sc.textFile("/tmp/*.gz");
	val lines=sc.textFile("hdfs://koreadev/tmp/movies2/");




- wholeTextFiles (디렉토리에 포함된 텍스트파일 읽어들이기)
        rdd가 생성되었다고 하더라고 해당디렉토리 파일에 읽기권한 이 없다면 에러가 발생한다.
        파티션의 갯수는 데이터가 인접한 상황에 따라 생성된다. 이를 제어 하기위해서는 2번째
        인자를 지정하여 생성할 수 있다.
  val lines=sc.wholeTextFiles("/tmp/*.dat", 3);
        val lines=sc.wholeTextFiles("c:\\dataset\\nba\\2014\\*");
- makeRDD	
- val rdd1 = sc.makeRDD(1 to 100)
- RDD 파티션 지정 생성 
- RDD textFile의 2번째 인자(파티션 갯수)
  파일에 대한 파티션의 갯수를 제어 하기 위해 두번째 인자를 사용한다.
val lines=sc.textFile("c:\\dataset\\movies.dat",256);

- RDD의 파티션당 1번만 작업 하기
 db 커넥션 등 파티션별로 한번만 작업해야 하는 경우 사용함
val rdd1=sc.parallelize(1 to 10, 3)
val rdd2=rdd1.mapPartitions(numbers=>
{
println("db  connection")
    numbers.map(number=>number +1)
}) 
rdd2.collect.foreach(println)

- RDD의 파티션 index를 이용한 작업
val rdd1 = sc.parallelize(1 to 10, 3)
val rdd2=rdd1.mapPartitionsWithIndex((idx,numbers) =>{
numbers.map{
     case number if idx ==1 => number+1
      case number if idx ==2 || idx ==0 => number+10
}
})
rdd2.collect.foreach(println)
- RDD Transformation
- map[U](f:(T) => U): RDD[U]
   문자열을 입력을 받아서 배열을 리턴 함
  - map은 각요소에 특정연산을 적용한 새로운 RDD를 생성합니다.
  - 숫자 RDD map을 이용 rdd 각요소에 연산하기
    val x=sc.parallelize(1 to 10)
val y=x.map(x => x*x) 
var z=x.map(_ + 1)
    var v=x.map(_.sum)
    
  - 문자 RDD를 map을 이용 변형 
var rdd=sc.parallelize(List("a","b","c","d"))
val charUP = words.map(x=>x.toUpperCase)	
val charLO = charUP.map(_.toLowerCase())
- map과 flatMap의 차이
- Map
map 결과는 줄을 " " 구분자로 분해한 결과를 array(줄)안의 array(단어)에 담는다.
즉 리턴값이 Array[Array[String]]
val rdd=sc.textFile("C:\\dataset\\1.txt").map(_.split(" "))
rdd.collect
res2: Array[Array[String]] = Array(Array(As, noted, by, @zero323,, makeRDD, has))
- flatMap 
flatMap결과는 각요소를 " " 구분자로 분해한 결과를 하나의 array 담는다.
즉 리턴값이 Array[String] 
val rdd=sc.textFile("C:\\dataset\\1.txt").flatMap(_.split(" "))
rdd.collect
    res2: Array[String] = Array(As, noted, by, @zero323,, makeRDD, has, 2, implementations.)
- filter 
  - 숫자 필터
val rdd=sc.parallelize(1 to 10)
val rdd1=rdd.filter(e=>e%2==0)
rdd1.collect	
res5: Array[Int] = Array(2, 4, 6, 8, 10)
- 페어 RDD 필터 적용
val rdd=sc.textFile("C:\\dataset\\apache.txt").flatMap(_.split(" ")).filter(x => x.nonEmpty).map(x=>(x,1)).reduceByKey(_+_)
val rdd1= rdd.map(x=>x.swap).sortByKey(ascending=false).map(x=>x.swap)
val rdd=sc.parallelize(1 to 15)
val rdd1=rdd.filter({case(x, y) =>(y>10)})

- RDD에 PIPE를 이용하여 외부 명령이용하기
val rdd=sc.parallelize(List("1,2,3","4,5,6","7,8,9"))
val result=rdd.pipe("cut, -f, 1,3 -d ,")
println(result.collect.mkString(", "))
- RDD에 숫자 요소의 값 누적 합산 하기 : reduce
val rdd1=sc.parallelize(1 to 10)
        val rdd2=rdd1.reduce(_+_)
println(rdd2)

- RDD에 문자 요소의 결합하기 : reduce
val rdd=sc.parallelize(List("a","b","c","d"),2)
val rdd2=rdd.reduce(_+_)
println(rdd2)

 - 페어 RDD에서 value에 대해서만 flatMapValue 적용하기
val rdd=sc.parallelize(List((1,"a,b"), (2,"a,b"), (3,"a,b")))
val rdd1=rdd.flatMapValues(v=>v.split(","))
rdd1.collect.foreach(println)
    아래의 결과처럼 value에 해당하는 값에 split이 적용되었다.
      Array[(Int, String)] = Array((1,a), (1,b), (2,a), (2,b), (3,a), (3,b))
- 페어 RDD에서 value에 대해서만 mapValue 적용하기
    val rdd1=sc.parallelize(List("a","b","c")).map((_,1))
    val rdd2=rdd1.mapValues(x=> x+10)
Array[(String, Int)] = Array((a,11), (b,11), (c,11))
- RDD의 contains를 이용한 문자열 filter 
   예제 1)
특정문자열이 포함된 라인을 찾을 때(x.contains)
 val rdd=sc.textFile("c:\\dataset\\apache.txt")
    val filter= rdd.filter(x => x.contains("LICEN"))
    res25: Array[String] = Array(" *    http://www.apache.org/licenses/LICENSE-2.0")
   예제 2)
특정문자열이 포함되지 않은 라인을 찾을 때(!x.contains)
val rdd=sc.textFile("c:\\dataset\\apache.txt")
    val filter= rdd.filter(x => !x.contains("LICEN"))
- union RDD 합치기	
val charLO = sc.parallelize(List("a","b","c","d","e"))
val charUP = sc.parallelize(List("A","B","C","D","E"))	
val unionRDD = charUP.union(charLO)
unionRDD.collect	
res8: Array[String] = Array(A, B, C, D, E, a, b, c, d, e)
- take : RDD의 일부데이터 화면출력
    unionRDD.take(10).foreach(println)	
    foreach(println)을 사용하면 array에 담겨있는 요소만 화면에 출력한다.
	
- collect : RDD의 전체 데이터 화면에 가져오기
    unionRDD.collect
    collect를 사용하기 위해서는 메모리에 올릴 수 있는 정도의 데이터 크기여야 한다.
    foreach(println)을 사용하면 array에 담겨있는 요소만 화면에 출력한다.
 - count: RDD의 줄수 세기
 - first: RDD의 첫줄 

- RDD 파일로 저장하기
    unionRDD.saveAsTextFile("/tmp/unionRDD.txt")  
		
- RDD 메모리에 저장하고 사용하기
- RDD가 Action으로 수행될 때마다 소스에서 부터 다시 로드되서 수행된다. 그래서 SPARK    매번 로드 해서 계산하여 사용하는 것을 방지하기위해 persist()와 cache() 함수를 제공한다.
디폴트는 메모리에 저장하고, 옵션으로 디스크를 지정할 수 있다. 디스크를 지정하면, 메모리에서 모지란 부분을 마치 swapping 하듯이 디스크에 데이타를 저장한다.
persist()와 cache()의 차이점은 cache()는 persist() 에서 저장 옵션을 MEMORY_ONLY로 한 옵션과 동일하다. RDD가 메모리나 디스크에 로드 되었다고 항상 로드된 상태로 있는 것이 아니다. 기본적으로 LRU (Least Recently Used) 알고리즘 (가장 근래에 사용되지 않은 데이타가 삭제되는 방식)에 의해서 삭제가 되가나, 또는 RDD.unpersiste() 함수를 호출하면 명시적으로 메모리나 디스크에서 삭제할 수 있다
MEMORY_ONLY : 
가장 빠른 옵션이다.
    MEMORY_ONLY_SER : 
Seriazlied 된 형태로 저장하기 때문에 메모리 공간은 줄일 수 있으나, 대신 CPU 사용률이 올라간다. 그래도 여전히 빠른 방식이다.
DISK_ONLY
데이타 양이 많을 경우에는 DISK에 저장하는 옵션보다는 차라리 Persist 를 하지 않고, 필요할때 마다 재계산 하는 것이 더 빠를 수 있다.
MEMORY_ONLY2
MEMORY_ONLY와 동일하지만 2개의 노드에 각각의 파티션을 복제한다.
OFF_HEAP 
메모리 클러스터를 이용하여, 서로 복제가 가능한 외부 메모리 클러스터에 저장하는 방식으로, JVM 상 메모리 보다는 성능이 약간 떨어지지만, 디스크보다는 빠르며, 큰 메모리 공간을 장애 대응에 대한 상관 없이 (자체 적으로 HA 기능을 제공함) 사용이 가능하다
MEMORY_AND_DISK_SER
여기에 특이한 점은, 메모리나 디스크에 저장할때, RDD를 RAW (원본 형식)으로 저장할 것인지 자바의 Serialized 된 형태로 저장할 지를 선택할 수 있다. ( Serealized 된 형태로 저장하기 MEMORY_ONLY_SER, MEMORY_AND_DISK_SER) 이렇게 저장하면, 메모리 사용량은 더 줄일 수 있지만, 반대로 저장시 Serizalied하는 오버로드와, 읽을때 De-Seriazlie 하는 오버로드가 더 붙어서 CPU 사용량은 오히려 증가하게 된다.
Serialized 로 저장하는 경우, 최대 약 4배 정도의 메모리 용량을 절약할 수 있으나, 반대로, 처리 시간은 400배 이상이 더 들어간다.

- word count 예제	
- foldByKey를 이용한 word count
  초기값을 전달할때 파티션 단위로 전달하기 때문에 초기값을 0이외에는 쓰지 
않는것이 좋다.
  val rdd1=sc.textFile("c:\\dataset\\apache.txt",5).flatMap(x=>x.split(" ")).map(x=>(x,1))
  val rdd3=rdd1.foldByKey(0)(_+_).map(x=>x.swap).sortByKey(ascending=false).map(x=>x.swap)
  rdd3.collect.foreach(println) 
 
- reduceByKey를 이용한 word count
  var rdd= sc.textFile("C:\\dataset\\apache.txt")
  var rdd1=rdd.flatMap(_.split(" "))
  var rdd2=word.map(word => (word,1))
  var result=rdd2.reduceByKey(_+_)
result.collect.foreach(println) 
- 마틴루터킹 목사의 연설문 word count
val speachRDD=sc.textFile("c:\\dataset\\martinlutherking.txt")
val wordRDD=speachRDD.flatMap(x=>x.split(" "))
val filterWord=sc.parallelize(List("the","I","a","and","we","and"
,"of","be","to","will","is","are","that","have","as","in","We"
,"our","with","this","not","from","one","go"))
val filterdWord=wordRDD.subtract(filterWord)
val wordCount=filterdWord.map(x=>(x,1)).reduceByKey(_+_)
.map(x=>x.swap).sortByKey(ascending=false).map(x=>x.swap)
rdd5.take(30)

- foldByKey를 이용한 word count
val rdd1=sc.textFile("c:\\dataset\\apache.txt")
val rdd2=rdd1.flatMap(x=>x.split(" ")).map((_,1))
val rdd3=foldByKey(0)(_ + _).map(x=>x.swap)
.sortByKey(ascending=false)
val rdd4=rdd3.map(x =>x.swap))
rdd4.collect.foreach(println)
- countByValue를 이용한 word count
리턴 타입이 페어 RDD가 아니다.  scala.collection.Map을 리턴한다.
       RDD 함수를 사용하기 위해서는 아래 과정을 거쳐 형변환을 해야 한다.
       val rdd=sc.textFile("c:\\dataset\\apache.txt").flatMap(_.split(" ")).countByValue()
val rdd2=sc.parallelize(rdd.toList).map(x=>x.swap).sortByKey(ascending=false)
- countByKey를 이용한 word count
리턴 타입이 페어 RDD가 아니다.  scala.collection.Map을 리턴한다.
 RDD 함수를 사용하기 위해서는 아래 과정을 거쳐 형변환을 해야 한다.
 countByValue와는 다르게 페어 RDD에만 적용이 가능하다.
 val rdd=sc.textFile("c:\\dataset\\apache.txt").flatMap(_.split(" ")).map(x=>(x,1)).countByKey()
val rdd2=sc.parallelize(rdd.toList).map(x=>x.swap).sortByKey(ascending=false)

- RDD에 라인번호 컬럼추가 : zipWithIndex
val rdd1 = sc.textFile("c:\\dataset\\apache.txt")
val rdd2 = rdd1.zipWithIndex().map(x=>x.swap)
rdd2.collect.foreach{ case(k,v) => println(s"${k},${v}")}


- 2개의 RDD 묶기 zip
 파티션의 개수가 동일해야 함. 요소의 수도 동일해야 함
val rdd1=sc.parallelize(1 to 5, 2)
val rdd2=sc.parallelize(List("a","b","c","d","e"), 2)
val rdd3=rdd1.zip(rdd2)
- 2개의 RDD 묶기 zipPartitions
파티션의 개수가 동일해야 함. 요소의 수는 동일하지 않아도 됩니다.
zipAll 함수는 iterator 결합하는데 사용한다. 컬렉션의 크기가 달라도 결합이 가능하다. 첫번째 iterator가 두번째 보다 요소가 더 많으면 zipAll함수는 첫번째 iterator의 나머지 요소에 empty라는 일종의 더미값을 결합한다.
2번째 요소가 더 많으면 -1을 사용한다.
val rdd1=sc.parallelize(1 to 10, 3)
      val rdd2=sc.parallelize(List("a","b","c","d","e"), 3)
      
      val rdd3=rdd1.zipPartitions(rdd2, true)((it1, it2) => {
          it1.zipAll(it2, -1, "empty")
          .map({case(x1,x2)=>x1+"-"+x2})
})

- 페어 RDD 평균구하기 reduceByKey
val scores = List(("math",100F), ("math",50F), ("eng",50F),("eng",60F), ("eng",90F))
val rdd = sc.parallelize(scores).map({case(key,value) =>(key,(value,1))})
val rdd2= rdd.reduceByKey({ case((value,cnt), (value1,cnt1)) =>(value+value1,cnt+cnt1) })
val rdd3= rdd2.map({ case(k, (v,c)) => (k, v/c ) })
rdd3.collect.foreach(println)

  - 페어 RDD 평균구하기 mapValue
val scores = List(("math",100F), ("math",50F), ("eng",50F),("eng",60F), ("eng",90F))       val rdd = sc.parallelize(scores) 
val rdd2=rdd.mapValues(value => (value, 1))
.reduceByKey {
        case ((sumL, countL), (sumR, countR)) => 
          (sumL + sumR, countL + countR)
}
.mapValues {
        case (sum , count) => sum / count 
}
      rdd2.collect.foreach(println)
- 페어 RDD 평균구하기 aggregateByKey
val scores = List(("math",100F), ("math",50F), ("eng",50F),("eng",60F), ("eng",90F))
val rdd1=sc.parallelize(scores)
val rdd2=rdd1.map({ case(k,v)=>(k,(v,1)) })
//평균값을 구하기 위해 점수의 합계와 개수를 연산 하기위해 아래와 같이 초기값 설정
val initval= ((0.0F,0))
val result=rdd2.aggregateByKey(initval)(
(ac: (Float,Int), ac1: (Float,Int)) => (ac._1 + ac1._1,  ac._2 +ac1._2), 
   (ac1: (Float,Int),ac2: (Float,Int)) => (ac1._1 +ac2._1, ac1._2 + ac2._2) 
).map({  case(key,value) => (key, value._1 / value._2) })
 result.collect.foreach(println)
- 페어 RDD 평균구하기 aggregateByKey Type 사용
val scores = List(("math",100F), ("math",50F), ("eng",50F),("eng",60F), ("eng",90F))
      
      type sumScore_cnt = (Float, Int)
      type key_SumScore_cnt = (String, (Float, Int))
     
      val rdd = sc.parallelize(scores)
      
      //컴바이너 생성
      val combiner = (score:Float) => (score,1)
      
      //파티션내 계산방법 정의
      val mergeValue = (sum1:sumScore_cnt, score:Float) =>{  
        (sum1._1 + score, sum1._2 + 1)
      }
      
      //파티션간 계산방법 정의
      val mergeCombiners = (col1:sumScore_cnt , col2:sumScore_cnt)=>{
        (col1._1 + col2._1 , col1._2 + col2._2)
      }
      
       val rdd2=rdd.combineByKey(combiner, mergeValue, mergeCombiners).map({
         case(k, (s, c)) => (k, s/c)
       })
       
      rdd2.collect().foreach(println)      

- 페어 RDD 평균구하기 aggregateByKey 사용자 정의 타입 사용
val score=List(ScoreAvg("math",100),ScoreAvg("math",80),ScoreAvg("eng",100),ScoreAvg("eng",80),ScoreAvg("eng",60))
      val scoreWithKey=for(i<-score) yield(i.subject,i)
      val rdd1=sc.parallelize(scoreWithKey)
      val initval= ((0.0F,0))
      val result=rdd1.aggregateByKey(initval)(
          (ac: (Float,Int), x: ScoreAvg) => (ac._1 + x.score, ac._2 +1), 
          (ac1: (Float,Int),ac2: (Float,Int)) => (ac1._1 +ac2._1, ac1._2 + ac2._2) ).map({
            case(key,value) => (key, value._1 / value._2)
          })
        result.collect.foreach(println)

// 사용자 정의 타입
case class ScoreAvg(var subject: String, var score: Float){}

- 페어 RDD 평균 구하기 aggregate 
   def aggregate[U](zeroValue: U)(seqOP: (U,T) => u, combOp: (U,U) =>U)(implicit arg0: ClassTag[U]): U
       ex 1)
      val input = sc.parallelize(1 to 10,3) 
      val result=input.aggregate((0, 0))((x, y) => (x._1 + y, x._2 + 1),
          (x,y) => (x._1 + y._1, x._2 + y._2))
      val avg = result._1 / result._2.toFloat
     ex 2)  
- aggregate
  rdd의 입력 데이터 타입과 action의 결과 타입이 다를 경우에 사용한다.
zeroValue은 파티션별로 계산을 수행할때 제공되는 초기값이다.
aggregate 첫번째 인자(seqOp)는 파티션 별로 수행되고 seqOp 첫번째 인자는 x는 total이고 
y는 카운트이다. aggregate 두번째 인자(combOp)는 파티션 별로 수행된 결과를 합쳐준다.
  첫번째 x는 누적변수이고 는 파티션별로 계산된 카운트값이다.
  

- 페어 RDD 에서 최대 값 구하기 fold
val employeeData = List(("Jack",1000.0),("Bob",2000.0),("Carl",7000.0),("Jack",2000.0),("Bob",3000.0),("Carl",4000.0))
val employeeRDD = sc.makeRDD(employeeData)
//fold를 사용하려면 시작 값이 필요합니다
val dummyEmployee = ("dummy",0.0);
//fold와 foldByKey의 차이는 key별로 그룹핑을 해서 그룹함수를 적용할 지 전체에 그룹함수를 적용할지의 차이이다.
val maxSalaryEmployee = employeeRDD.fold(("dummy",0.0))((acc,acc2) =>  if(acc._2 < acc2._2) acc2 else acc)
println("employee with maximum salary is"+maxSalaryEmployee)

- 페어 RDD 에서 그룹 중 최대 값 구하기 foldByKey
val deptEmployees = List(
      ("cs",("jack",1000.0)),
      ("cs",("bron",1200.0)),
      ("phy",("sam",2200.0)),
      ("phy",("ronaldo",500.0))
  )
  val employeeRDD = sc.makeRDD(deptEmployees)
  val maxByDept = employeeRDD.foldByKey(("dummy",0.0))((acc,acc1)=> if(acc._2 > acc1._2) acc else acc1)
maxByDept.collect

- 페어 RDD 에서 그룹 중 구하기 fold
val rdd1=sc.parallelize( 1  to 10)
val rdd2=rdd1.groupBy{
case i: Int if(i % 2==0) => "even"
case _ => "odd"
}
rdd2.collect.foreach(println)

- RDD를 키 값으로 두부분으로 구별하기 GroupBy
val rdd1=sc.parallelize( 1  to 10)
val rdd2=rdd1.groupBy{
case i: Int if(i % 2==0) => "even"
case _ => "odd"
}
rdd2.collect.foreach(println)



- 하나의 RDD에서 같은 요소로 묶기 GroupByKey
  패어 RDD에만 적용가능하다.
val rdd=sc.parallelize(List("a","a","b","b","c","c","d")).map((_,1))
val rdd1=rdd.groupByKey
rdd1.collect.foreach(println)

- 2개의 RDD에서 같은 키값으로 JOIN하기 
val rdd1=sc.parallelize(List("kang","jung","hwa","hyun","jun")).map(x => (x,1))
      val rdd2=sc.parallelize(List("hyun","jun")).map(x => (x,2))
      val rdd3= rdd1.join(rdd2)

- 2개의 RDD에서 같은 키값으로 leftOuterJoin 하기
val rdd1=sc.parallelize(List("kang","jung","hwa","hyun","jun")).map(x => (x,1))
val rdd2=sc.parallelize(List("hyun","jun")).map(x => (x,2))
val rdd3= rdd1.leftOuterJoin(rdd2)
rdd3.collect

RDD1	Kang	jung	Hwa	Hyun	jun
RDD2				Hyun	Jun
rdd1.leftOuterJoin(rdd2)	(kang,(1,None))	(jung,(1,None))	(Hwa,(1,None))	(hyun,(1,Some(2)))	(Jun,(1,Some(2)))

- 다수의 RDD 연산
- 2개의 RDD Catesian
   결과는 rdd1의 개수 * rdd2의 개수만큼 페이RDD를 반환한다.
  (1,1),(1,2),(1,3) ……..
val rdd1=sc.parallelize(1 to 10)
    val rdd2=sc.parallelize(1 to 5)
    val rdd3=rdd1.cartesian(rdd2)
    rdd3.collect.foreach(println)

- 2개의 RDD 같은키를 이용해서 묶기 cogroup
   리턴타입은 아래와 같다
   RDD[(Int, (Iterable[String], Iterable[String]))]
   아래처럼 12번 나온 단어끼리 묶어 준다.
   12,[the],[the]
   3,[this, under, Netty, in, for, to],[this, under, Netty, in, for, to]
val rdd1=sc.textFile("c:\\dataset\\apache.txt").flatMap(x=>x.split(" ")).filter(x=>x.nonEmpty).map(x=>(x,1)).reduceByKey(_ + _).map(x=>x.swap)
val rdd2=sc.textFile("c:\\dataset\\apache.txt").flatMap(x=>x.split(" ")).filter(x=>x.nonEmpty).map(x=>(x,1)).reduceByKey(_ + _).map(x=>x.swap)
val rdd3=rdd1.cogroup(rdd2).sortByKey(ascending=false)
      rdd3.collect.foreach {
        case(k,(v1,v2)) =>{
          println(s"${k},[${v1.mkString(", ")}],[${v2.mkString(", ")}]")
        }
      }
- 2개의 RDD에서 유일한 요소만 반환하기 distinct
val rdd1=sc.textFile("c:\\dataset\\2.txt").flatMap(x=>x.split(" "))
     val rdd2=sc.textFile("c:\\dataset\\3.txt").flatMap(x=>x.split(" "))
     val rdd3=rdd1.union(rdd2).distinct()

- 2개의 RDD에서 차집합 구하기 subtractByKey
  페어 RDD에만 사용가능
val rdd1=sc.parallelize(List("a","b","c","d","e")).map(x=>(x,1))
val rdd2=sc.parallelize(List("a","b","f")).map(x=>(x,2))
val rdd3=rdd1.subtractByKey(rdd2)

- 2개의 RDD에서 차집합 구하기 subtract
  페어 RDD가 아닌 경우에만 사용가능
val rdd1=sc.parallelize(List("a","b","c","d","e"))
val rdd2=sc.parallelize(List("a","b","f","g","h"))
val rdd3=rdd1.subtract(rdd2)


SPARK ACTION 연산

- take (n) 
    retrun type : array
	RDD 데이터의 일부를 배열 가져온다.
	lines.take(3)
	아래처럼 처음 조회되는 3줄을 Array에 담아 온다.
	res4: Array[String] = Array(The first query shows that Curry makes, 
							on average, more three-pointers than anyone in the history of the NBA., 
							The second query shows that Joe Hassett had the best three-point shooting season in 1981,, 
							as compared to the rest of the league, in the history of the NBA. Curry doesnven rank in the top 10. (He barely misses it, coming in at 12th.)
							)
   
- first (액션)
    retrun type : string
	데이터의 첫라인을 가져온다.
	lines.first()
	res14: String = The first query shows that Curry makes, on average, more three-pointers than anyone in the history of the NBA.   
   
- count 파일의 줄 세기(액션)
    retrun type : Long
	lines.count()
	res2: Long = 6   

- collect (액션)
    retrun type : array
	RDD의 전체데이터를 가져온다.
	데이터 집합이 너무 크면 사용할 수 없다. 메모리에 올릴수 있는 정도여야 한다.
	이러 경우는 hdfs나 아마존 S3와 같은 분산파일 시스템에 쓰는것이 일반적이다.
	filter를 통해 작은 데이터 집합을 만든후 분산이 아닌 로컬에서 
	데이터를 처리하고 싶을때 사용한다.
	lines.collect

- filter 예제 (transform)
  
  split은 레귤러 익스프레션을 지원한다.
  //아래는 알파벳이 아닌것을 기준으로 분할한다.
  val words2 = lines.flatMap(line => line.split("[^a-zA-Z]+")).filter(word => word.nonEmpty)
  
  //아래는 알파벳과 숫자가 아닌것을 기준으로 분할한다.
  val words3 = lines.flatMap(line => line.split("[^a-zA-Z0-9]+")).filter(word => word.nonEmpty)
  
  //단어의 길이가 4 이상이 단어만 필터 한다.
  val words4 = lines.flatMap(line => line.split("[^a-zA-Z0-9]+")).filter(x => x.length >= 4)
  
  //단어의 길이가 4가 아닌 단어만 필터 한다.
  val words4 = lines.flatMap(line => line.split("[^a-zA-Z0-9]+")).filter(x => x.length != 4)
  
  //단어의 길이가 4인 단어만 필터 한다.
  val words4 = lines.flatMap(line => line.split("[^a-zA-Z0-9]+")).filter(x => x.length == 4)
  
  //contains를 이용한 필터
  val inRDD = lines.filter(_.contains("in"))
  val inRDD2 = lines.filter(line => line.contains("in"))
	
- top n 사용 예제
   scala> val input1 = sc.parallelize(List(1,2,3,4))
   input1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at <console>:24
   scala> input1.top(4)
   res6: Array[Int] = Array(4, 3, 2, 1)  
  
	
- 지정된 숫자 만큼 배열 가져온다.
  원하는 데이터를 순서대로 가져오지 않는다.
  특정 파티션의 데이터만을 가져올 수도 있다.
  words.take(10)

- foreach
  rdd 형식은 생략하고 해당 요소만 출력한다.
  foreach는 괄호 안의 각요소에 적용한다.
  lines.take(5).foreach(println)

- RDD의 저장방법
lines.saveAsTextFile("C:\\hadoop\\spark_notebook\\nba2.txt")
lines.saveAsSequenceFile("C:\\hadoop\\spark_notebook\\nba2")
	
- 두 가지 RDD에 대한 연산 union, intersaction
    val input1 = sc.parallelize(List(1,2,3,4))
	val input2 = sc.parallelize(List(3,4,5,6))
	
	- 합집합
	val unionRDD = input1.union(input2)
	Array[Int] = Array(1, 2, 3, 4, 3, 4, 5, 6)
	- 교집합
	val interRDD = input1.intersection(input2)
	Array[Int] = Array(4, 3)
	
	- 차집합
	val subRDD = input1.substract(input2)
	Array[Int] = Array(2, 1)
	
	- cartesian(4*4)
	val cartRDD = input1.catesian(input2)
	Array[(Int, Int)] = Array((1,3), (1,4), (2,3), (2,4), (1,5), (1,6), (2,5), (2,6), (3,3), 
	                          (3,4), (4,3), (4,4), (3,5), (3,6), (4,5), (4,6))
	
- RDD persist
   RDD에 대해 persist를 요청하면 RDD를 계산한 노드들은 
   계산된 값을 파티션에 저장하고 있게 된다.
   persist는 액션을 수행하기 전에 호출되어야 한다.
   RDD는 캐시에서 데이터를 삭제 할 수 있도록 unpersist 메소드도 지원한다.
   
   val result=input.map(x => x*y)
   result.pesist(StorageLevel.MEMORY_ONLY)
   result.pesist(StorageLevel.MEMORY_ONLY_SER)
   result.pesist(StorageLevel.MEMORY_AND_DISK)
   result.pesist(StorageLevel.MEMORY_AND_DISK_SER)
   result.pesist(StorageLevel.DISK_ONLY)


  
- distributeStatus FUNC 사용
def myfunc(index:Int,iter:Iterator[(Int)]):Iterator[String]={
 iter.toList.map(x => "[partID]:"+index+",val:"+x+"]").iterator
}
  
  z.mapPartitionsWithIndex(myfunc)
  z.aggregate(0)(math.max(_,_),_+_)
===========================================================================
dataframe, dataset
===========================================================================
/***************************************************
**case class를 이용한 스키마 정의
**case class를 이용한 rdd의 정의
**RDD 데이터의 표출
**RDD는 show가 없고 take 메소드를 사용해야 한다.
**show는 dataset에서 사용가능하다.
**RDD  dataset변환(toDS)
**dataset에 where filter적용 후 select
**dataset 형변환
****************************************************/











SPARK 데이터프레임
- 데이터프레임
아파치 스파크 1.3 버전에서 데이터프레임이 처음 소개되었습니다. 이러한 데이터프레임은 텅스텐 프로젝트를 통해 스파크 1.6 버전에서 많은 성능향상을 이루어 냈습니다. 
RDD는 분산환경에서 메모리에 데이터를 로딩하여 빠르고 안정적으로 동작하는 프로그램을 작성할 수 있었습니다. 기존의 Hadoop을 통해 분산 데이터 처리를 하던 개발자 입장에서는 수행속도가 매우 빠르고 MapReduce에 비해 상대적으로 익히기 쉬운 장점을 가지고 있고 비정형데이터를 정형화하기 위한 많은 기능을 제공합니다. 하지만 빅데이터 처리에 처음 입문하는 입장에서는 scala, python, java중 하나를 선택하여 API를 학습하여 개발하여야 한다는 부담감이 있는 것도 사실이었습니다. 또 RDD를 사용하면 사용자의 코팅 방식 즉 필터와 조인을 거는 순서 등에 따라 현격하게 성능차이를 보입니다.
이러한 단점을 보안하여 데이터프레임은 분산된 데이터 컬렉션에 스키마 개념을 도임함 으로서 스파크 사용자는 스파크 SQL로 정형화된 데이터를 SQL이나 함수를 통해 데이터 분석을 가능하게 합니다. 또한 데이터프래임은 사용자의 최적화 되지않으 SQL을 SPARK SQL 엔진인 카탈리스트 옵티마이저를 통해 비용기반 최적화를 달성하게 해줍니다.
사용자가 RDD를 사용하기 위해 SparkContext가 필요한 것처럼 Spark 2.0에서 DataFrame또는 DataSet을 사용하기 위해서는 SparkSession이 필요합니다. 
아래 화면은 Spark 2.0에서 spark-shell을 수행 했을 때 화면 입니다.SparkContext가 “sc”로 표현되는 것처럼 SparkSession은 “spark”으로 정의됩니다.
 
SparkSession은 sparksql을 사용하기 위한 진입점입니다. SparkContext가 하나만 생성된는 것에 비해 SparkSession은 여러개를 생성할 수 있다. SparkSession을 종료 하기위해서는 spark.stop 명령어를 사용한다.
https://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset/
- 데이터셋
데이터셋은 Spark 1.6 alpah버전에서 처음소개되었으면 Spark 2.0으로 Dataset도 DataFrame과 마찬가지로 Catalyst Optimizer를 통해 실행 시점에 코드 최적화를 통해 성능을 향상하고 있다. Datasets은 JVM객체에 대한 바인딩으로 Encoder를 사용하여 유형별 JVM객체를 Tungsten의 내부 메모리 표현으로 매핑하게 된다. 결과적으로 Encoder를 통해 JVM객체를 효율적으로 직렬화/비직렬화 할 수 있으며 소형의 바이트 코드를 생성하게 됨으로 이는 cache size 축소 및 처리시간 관련 성능상 이점을 가질 수 있다. 
RDD, DataFrame, Datasets사용기준
https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html
필자가 생각하는 기준은 아래와 같습니다. RDD는 정형화되지 않은 데이터를 정형화하기위해 작업을 하기위해서는 RDD가 작업하기에 적합합니다. 사용자가 직접 낮은 수준의 API를 통해 작업성능의 튜닝이 가능하다면 RDD가 적합합니다. 
함수형 프로그래밍 구조로 데이터를 조작하는데 익숙한 사용자에게 적합합니다.
RDD와는 달리, 데이터프레임은는 관계형 데이터베이스의 테이블처럼 스키마와 ROW와 컬럼으로 구성됩니다. 그렇기 때문에 데이터 엔지니어를 초월하여 Spark을 더 많은 사람들이 이용할 수 있게 합니다.
스파크 2.0에서 개념적으로, DataFrame 을 일반 객체의 집합 인 DataSet[Row]으로 이루이진 집합으로 간주 합니다.  즉 스파크 2.0에서 Dataset이라는용어는 아래의 그림과 같이 DataFrame과 Dataset을 포함한 통칭으로 간주되기도 합니다. 
이미 데이터가 정형화 되어 있고 결과의 도출에만 초점을 맞추고 성능은 시스템이 알아서 해주기 원한다면 Catalyst Optimizer를 통해 성능최적화가 가능한 DataFrame, Datasets을 고려해 볼 수 있습니다.
DataFrame, Datasets중에는 Scala, Java개발자라면 Encoder의 장점을 활용하는 Datasets이 적합하다고 생각합니다.
RDD, DataFrame, Datasets는 API 메소드 호출을 통해 DataFrame 또는 Dataset 및 RDD 간에 원활하게 변환이 가능하면 DataFrames 및 Dataset은 RDD 위에 구축됩니다.
 


자바의 직렬화 프레임워크나 kyro와 같이 자바객체와 바이너리 포멧간의 변환을 처리하는 용도로 사용된다.
Kryo는 바이트 배열의 크기를 줄이기 위해 압축을 지원합니다. Kryo 는 속도, 효율성 및 사용자 친화적 인 API에 중점을 둔 Java 직렬화 프레임 워크입니다
- 인코더는 기존 직렬화 방식과는 다르게 스파크sql내부에서 사용하는(텅스텐엔진) 
- 바이너리 포멧을 사용함으로써 연산성능과 메모리 효율을 높이는 장점이 있습니다.
- 데이터셋을 사용하기위해서는 데이터셋 내부 데이터타입에 대한 encoder를 반드시 
- 지정해야 합니다. 스팍의 경우는 문자열이나 정수 등 기본타입에 대한 인코더 정보를 암묵적
- 으로 변환 방식을 이용해서 제공하기 때문에 스파크세션의 implicits객체를 임포트 하면
- 별도의 인코더를 지정하지 않고 사용할 수 있습니다.








DataSet 생성
val data = 1 to 10 toList
val ds = data.toDS
val result=ds.map(x=>x+1)
val result=ds.select(col("value")+1).show
val spark = SparkSession ..... 을 정의 해주고 난후에 uil
“import spark.implicits._”을 해줘야 한다.

import org.apache.spark.sql.functions._
import org.apache.spark.sql.{Dataset, Encoders, SparkSession}
val spark = SparkSession
      .builder()
      .appName("DatasetSample")
      .master("local[*]")
      .config("spark.driver.host", "127.0.0.1")
      .getOrCreate()
import spark.implicits._
val list=List(("1","2","3"),
               ("4","5","6"),
               ("7","8","9"))

val ds = spark.createDataset(list)
ds.show
- 데이터셋 생성
- 아래의 예제는 0부터 4까지의 숫자타입의 데이터 셋을 생성한다.
scala> val dataset = spark.range(5)
dataset: org.apache.spark.sql.Dataset[Long] = [id: bigint]
//filter operator accepts a Scala function
dataset.filter(n => n % 2 == 0).count

- DataSet에 sql 날리기
// case class를 사용해서 schema를 정의합니다.
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.SparkContext
import org.apache.spark.sql.functions._
import org.apache.spark.sql.{Dataset, Encoders, SparkSession}
import org.apache.spark.rdd.RDD
      
case class Family(name: String, age: Int, job: String)
object UsingSqlToDs {
  def main(args : Array[String]){
      val gsc=new GetSparkContext
      val sc= gsc.getSparkContext()
      val familyRDD: RDD[Person] = sc.parallelize(Seq(Family("Hyunhun", 10, "son")))
    val spark = SparkSession
      .builder()
      .appName("UsingSqlToDs")
      .master("local[*]")
      .config("spark.driver.host", "127.0.0.1")
      .getOrCreate()      
      import spark.implicits._
      val family = familyRDD.toDS
      val child = people.where('age >= 10).where('age <= 19).select('name)
      teenagers.show   
      spark.stop
   }
}

- DataSet에 sql 질의문 예제
val child = family.where('age >= 0).where('age <= 20).select('name,'age)
val child = family.where('age >= 0).where('age <= 20).select('age).as[String]
val child = family.where('age >= 0).where('age <= 20).select('name).as[String]
- Dataset을 이용한 TempView 생성
people.createOrReplaceTempView("people")	
- TempView를 이용한 sql 수행
val teenagers = sql("SELECT * FROM people WHERE age >= 10 AND age <= 19")
teenagers.show
- sql 함수의 사용하여 csv를 이용한 dataset 생성
spark.sql("CREATE OR REPLACE TEMPORARY VIEW v1 (rank int, name string, position string) USING csv OPTIONS (path='C:\\\\dataset\\\\data\\\\2017\\\\2017.csv', header='true')")
spark.sql("FROM v1").show
+----+--------------------+--------+
|rank|                name|position|
+----+--------------------+--------+
|   1|Alex Abrines\abri...|      SG|
|   2|  Quincy Acy\acyqu01|      PF|
|   2|  Quincy Acy\acyqu01|      PF|
|   2|  Quincy Acy\acyqu01|      PF|
+----+--------------------+--------+

spark.sql("SELECT * FROM v1 WHERE rank> 10").show
spark.sql("show tables").show
- 테이블 select row 수 제한
spark.sql("select * from v1 limit 10").show
- 테이블 description확인
spark.sql("desc EXTENDED v1").show

- spark session의 생성
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
.appName("My Spark Application") 
.master("local[*]")
.enableHiveSupport() 
.config("spark.sql.warehouse.dir", "target/spark-warehouse")
.getOrCreate
- spark에서 hive 데이터베이스 내의 테이블을 그대로 읽어 들일 수 있다.
- 뿐만아니라 temporary 테이블도 읽을 수 있다.
spark.catalog.listTables.show

- sql 함수를 사용하면 hive에서 사용할 수 있는 모든 형식의 sql을 사용할 수 있다.
sql("DROP TABLE IF EXISTS type_test")
- Empty Dataset 생성
val strings = spark.emptyDataset[String]
strings: org.apache.spark.sql.Dataset[String] = [value: string]
- 테이블의 스키마 구조 출력
strings.printSchema
root
 |-- value: string (nullable = true)

- Dataset의 생성
방법 1)
val one = spark.createDataset(Seq(1))
one: org.apache.spark.sql.Dataset[Int] = [value: int]
방법 2)
val one = Seq(1,2,3,4,5).toDS
one: org.apache.spark.sql.Dataset[Int] = [value: int]
방법 3)
range 메소드 사용
//step의 숫자만큼 증가하고 end보다 작은 dataset을 생성한다.(15는 포함되지 않음)
spark.range(start = 0, end = 15, step = 1, numPartitions = 5).show
spark.range(start = 0, end = 16, step =2 , numPartitions = 5).show
방법 4)
텍스트 파일로부터 생성
val source="c:\\dataset\\data\\2017\\2017.csv"
val df=spark.read.text(source)
- DataFrame을 이용한 word count
explode 함수는 배열에 포함된 요소를 하나의 컬럼을 가진 Row로 변형시켜주는 역할을 합니다. 
val source="c:\\dataset\\martinlutherking.txt"
val df=spark.read.text(source)
val wordDF=df.select(explode(split(col("value")," ")).as("word"))
val result = wordDF.groupBy("word").count
result.show



- DataSet을 이용한 word count
val source="c:\\dataset\\martinlutherking.txt"
val ds=df.as[(String)]
val wordDF=ds.flatMap(_.split(" "))
val result = wordDF.groupByKey(v => v).count
result.show
- 처리결과의 저장소에 저장
result.write.text(“c:\\dataset\result.txt”)

- DataFrame 생성하기
방법 1) 여러가지 파일 소스로부터 생성 방법
val df = spark.read.format("com.databricks.spark.csv").option("header", "true").load("/tmp/movies2.dat")
val df= spark.read.format("json").load("/tmp/score.json").select("name", "score").where($"score" > 15).write.format("csv").save("/tmp/output-csv1") 
val df = sqlContext.read.format("com.databricks.spark.xml").option("rowTag", "book").load("books.xml")
방법 2) List Seq 사용
List Seq로부터 데이터프레임을 생성할 수 있으나 Array는 사용할 수 없다.
Seq((1, 2),(2, 3),(3, 4),(2, 4),(3, 5)).toDF("i", "j").show
List(("dog","walwal"),("cat", "yaongyaong"),("duck", "kwakwak")).toDF("i", "j").show

방법 2) 데이터 프레임과 type safety 
아래의 코드를 수행해보면 printSchema로 찍어보면 첫번째 컬럼이 String임에도 불구하고 df.first()(0) 첫번째 요소를 찍어보면 type이 any로 찍히는 것을 볼 수 있습니다. 즉 Type Safety를 읽어 버립니다. 그렇기 때문에 printSchema에서 확인한 정보를 가지고 해당 컬럼을 연산하고자 할 때 컴파일시에는 에러가 나지 않으나 런타임에서 에러가 발행할 수 있습니다. 즉 개발자가 IDE를 통해 coding하는순간에는 에러가 발생하지 않지만 배포를 다해서 실행할 때 에러가 나기 때문에 개발자가 에러를 디버깅하는데 단점이 될 수 있습니다. 
type-safety란 compile-time에 value에 대해 알고 있는 것을 이용하여, 대부분의 실수의 결과를 최소화 하는 것이다

case class Family(name: String, age: Int, job: String)
val row1= Family ("Hyunji", 10, "Daughter")
val row2= Family ("Hyunjun", 13, "Son")
val row3= Family ("Heakyung", 5, "Mother")
val row4= Family ("junghwa", 13, "Father")
val data = List(row1,row2,row3,row4)

val df= spark.createDataFrame(data)
df.printSchema
df.first()(0)

root
|-- name: string (nullable = true)
|-- age: integer (nullable = false)
|-- job: string (nullable = true)
res6: Any = Hyunji
val ds = data.toDS
-	아래는 위의 예제를 데이터셋을 통해 수행 했을 때의 결과 입니다.
case class Family(name: String, age: Int, job: String)
val row1= Family ("Hyunji", 10, "Daughter")
val row2= Family ("Hyunjun", 13, "Son")
val row3= Family ("Heakyung", 5, "Mother")
val row4= Family ("junghwa", 13, "Father")
val data = List(row1,row2,row3,row4)

val ds = data.toDS
ds.printSchema
ds.first.name


root
|-- name: string (nullable = true)
|-- age: integer (nullable = false)
|-- job: string (nullable = true)

res20: String = Hyunji

- 데이터 프레임으로부터 사용

// GROUP BY 수행
val query = df.groupBy('i).agg(sum('j)).as[(Int, BigInt)]
query.show;  리턴타입이 데이터 셋이다.
query.take(10)  리턴타입이 Array이다.
res0: Array[(Int, BigInt)] = Array((1,2), (3,9), (2,7))
- Order By 수행 : as("max")는 컬럼명 알리아스 이다.
val query = df.groupBy('i).agg(max('j).as("max")).orderBy("max").as[(Int, Int)]
val query = df.groupBy('i).agg(min('j).as("min")).orderBy(desc("min")).as[(Int, Int)]
//결과 타입을 정하기 애매한 경우 지정하지 않으면 spark이 알아서 정의해 준다.
val query = df.groupBy('i).agg(avg('j).as("avg")).orderBy("avg")
val query = df.groupBy('i).agg(sum('j).as("sum")).orderBy("sum")

- df에서 TempView 사용하기
val df = Seq((1, 1), (-1, 1),(2,3)).toDF("key", "value")
df.createOrReplaceTempView("src")
- select 절에서 if 사용 a < 0 작으면 a를 리턴 크거나 같으면 b을 리턴
sql("SELECT IF(a < 0, 'a','b') FROM (SELECT key a FROM src) temp").show













#UDF 의 등록 방법
#Accessing UDF Registration Interface — udf Attribute
//step 1 udf 등록
spark.udf.register("myUpper", (s: String) => s.toUpperCase)
//step 2 dataset 생성 a~c를 가진 strs이름을 가진 dataset을 생성
val strs = ('a' to 'c').map(_.toString).toDS
//step 3 테이블 등록
strs.registerTempTable("strs")
//step 4 UDF의 사용
scala> sql("SELECT *, myUpper(value) UPPER FROM strs").show
+-----+-----+
|value|UPPER|
+-----+-----+
|    a|    A|
|    b|    B|
|    c|    C|
+-----+-----+

# hive의 테이블을 이용하여 df 만들기
scala> val df = spark.table("type_test")
df: org.apache.spark.sql.DataFrame = [str_col: string, date_col: date ... 5 more fields]

scala> df.show
+-------+----------+--------------------+-------+--------+----+----+
|str_col|  date_col|           timps_col|dec_col|dec2_col|  c1|  c2|
+-------+----------+--------------------+-------+--------+----+----+
|   kang|2016-01-02|2016-02-02 02:02:...|     45|45.66000|null|    |
|   hwa1|      null|2016-02-02 02:02:...|   null|45.77778|null|null|
+-------+----------+--------------------+-------+--------+----+----+

- spark shell에서 context는 하나만 존재할 수 있지만
- SparkSession은 여러개 존재할 수 있다.
- spark shell을 띄우면 spark context는 sc라는 이름으로
- spark session은 spark라는 이름으로 정의된다.
- rdd를 사용하기위해 Spark context가 필요한 것 처럼
- 데이터프레임을 생성하기위해서는 Spark session이 필요하다.
- Spark context available as 'sc' (master = yarn, app id = application_1509580256251_2539).
- Spark session available as 'spark'.
scala> val newSession = spark.newSession
newSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@122f58a
- spark session의 종료
scala> newSession.stop()

- spark session간 공유가 되는 SharedState  
- SharedState — Shared State Across SparkSessions
- SharedState takes a SparkContext when created
Name                     Type 
-----------------------------------------------
cacheManager             CacheManager
externalCatalog          ExternalCatalog
globalTempViewManager    GlobalTempViewManager
jarClassLoader           NonClosableMutableURLClassLoader
listener                 SQLListener
sparkContext             SparkContext
warehousePath


- 데이터 프레임에 대한 plan 출력하기
//방법 1
scala> spark.range(1).filter('id === 0).explain(true)
//방법 2
scala> spark.range(1).filter(_ == 0).explain(true)
== Parsed Logical Plan ==
'Filter ('id = 0)
+- Range (0, 1, step=1, splits=Some(2))

== Analyzed Logical Plan ==
id: bigint
Filter (id#41L = cast(0 as bigint))
+- Range (0, 1, step=1, splits=Some(2))

== Optimized Logical Plan ==
Filter (id#41L = 0)
+- Range (0, 1, step=1, splits=Some(2))

== Physical Plan ==
*Filter (id#41L = 0)
+- *Range (0, 1, step=1, splits=Some(2))

- 데이터 셋 생성시 아래 단계를 거쳐 생성된다.
  STEP 1. SparkSession
  STEP 2. QueryExecution
  STEP 3. Encoder for the type T of the records

- 데이터 셋의 형변환
toDS(): Dataset[T]
toDF(): DataFrame
toDF(colNames: String*): DataFrame


case class Token(name: String, productId: Int, score: Double)
val data = Seq(
Token("aaa", 100, 0.12),
Token("aaa", 200, 0.29),
Token("bbb", 200, 0.53),
Token("bbb", 300, 0.42))

- 데이터 to 데이터셋
val ds = data.toDS

- 데이터 to 데이터프레임
val df = data.toDF

- 데이터프래임을 데이터셋에 삽입
val ds = df.as[Token]

- 데이터셋의 스키마 표시
ds.printSchema

- 데이터프래임의 스키마 표시
df.printSchema

-현재 작업하고 있는 row의 instance를 표시해 준다.
df.map(_.getClass.getName).show(false)
ds.map(_.getClass.getName).show(false)

- Encoders클래스의 사용
- 자바의 직렬화 프레임워크나 kyro와 같이 자바객체와 바이너리
- 포멧간의 변환을 처리하는 용도로 사용된다.
- 인코더는 기존 직렬화 방식과는 다르게 스파크sql내부에서 사용하는(텅스텐엔진) 
- 바이너리 포멧을 사용함으로써 연산성능과 메모리 효율을 높이는 장점이 있습니다.
- 데이터셋을 사용하기위해서는 데이터셋 내부 데이터타입에 대한 encoder를 반드시 
- 지정해야 합니다. 스팍의 경우는 문자열이나 정수 등 기본타입에 대한 인코더 정보를 암묵적
- 으로 변환 방식을 이용해서 제공하기 때문에 스파크세션의 implicits객체를 임포트 하면
- 별도의 인코더를 지정하지 않고 사용할 수 있습니다.

방법 1 : 인코더를 사용
// The domain object for your records in a large dataset
case class Person(id: Long, name: String)
import org.apache.spark.sql.Encoders
scala> val personEncoder = Encoders.product[Person]
       personEncoder: org.apache.spark.sql.Encoder[Person] 
       = class[id[0]: bigint, name[0]: string]

scala> personEncoder.schema
       res0: org.apache.spark.sql.types.StructType 
	   = StructType(StructField(id,LongType,false), 
	   StructField(name,StringType,true))
scala> personEncoder.clsTag
       res1: scala.reflect.ClassTag[Person] = Person
       import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
scala> val personExprEncoder = personEncoder.asInstanceOf[ExpressionEncoder[Person]]
       personExprEncoder: org.apache.spark.sql.catalyst.encoders.ExpressionEncoder[Person] =
       class[id[0]: bigint, name[0]: string]
// ExpressionEncoders may or may not be flat
scala> personExprEncoder.flat
      res2: Boolean = false
// The Serializer part of the encoder
scala> personExprEncoder.serializer
       res3: Seq[org.apache.spark.sql.catalyst.expressions.Expression] 
	   = List(assertnotnull(input[0, Person, true], 
	   top level non-flat input object).id AS id#0L, staticinvoke
	   (class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, 
	   assertnotnull(input[0, Person, true], top level non-flat input object).name, true) AS name#1)
// The Deserializer part of the encoder
scala> personExprEncoder.deserializer
       res4: org.apache.spark.sql.catalyst.expressions.Expression 
	   = newInstance(class Person)
scala> personExprEncoder.namedExpressions
       res5: Seq[org.apache.spark.sql.catalyst.expressions.NamedExpression] 
	   = List(assertnotnull(input[0, Person, true], 
	   top level non-flat input object).id AS id#2L, 
	   staticinvoke(class org.apache.spark.unsafe.types.UTF8String, 
	   StringType, fromString, assertnotnull(input[0, Person, true], 
	   top level non-flat input object).name, true) AS name#3)
// A record in a Dataset[Person]
// A mere instance of Person case class
// There could be a thousand of Person in a large dataset
val jacek = Person(0, "Jacek")
// Serialize a record to the internal representation, i.e. InternalRow
scala> val row = personExprEncoder.toRow(jacek)
       row: org.apache.spark.sql.catalyst.InternalRow = [0,0,1800000005,6b6563614a]
// Spark uses InternalRows internally for IO
// Let's deserialize it to a JVM object, i.e. a Scala object
import org.apache.spark.sql.catalyst.dsl.expressions._
// in spark-shell there are competing implicits
// That's why DslSymbol is used explicitly in the following line
scala> val attrs = Seq(DslSymbol('id).long, DslSymbol('name).string)
       attrs: Seq[org.apache.spark.sql.catalyst.expressions.AttributeReference] 
	   = List(id#8L,Encoders — Internal Row Converters 63 name#9)
scala> val jacekReborn = personExprEncoder.resolveAndBind(attrs).fromRow(row)
       jacekReborn: Person = Person(0,Jacek)
// Are the jacek instances same?
scala> jacek == jacekReborn
       res6: Boolean = true




방법 2 : 인코더를 지정하지 않고 사용
case class Person(id: Long, name: String)
import spark.implicits._
val row1=(1,"kang")
val row2=(2,"jung")
val row3=(3,"hwa")
val data=List(row1,row2,row3)
val ds=spark.createDataset(data)


scala> val personEncoder = Encoders.product[Person]
personEncoder.schema

val jacek = Person(0, "Jacek")
val row = personExprEncoder.toRow(jacek)


- createDataFrame을 사용한 데이터프레임 생성
val lines = sc.textFile("/tmp/movies2.dat")
lines: org.apache.spark.rdd.RDD[String] = /tmp/movies2.dat MapPartitionsRDD[3] at textFile at <console>:24
val headers = lines.first
headers: String = id,title,genre
import org.apache.spark.sql.types.{StructField, StringType}
val fs = headers.split(",").map(f => StructField(f, StringType))
import org.apache.spark.sql.types.StructType
val schema = StructType(fs)
val noheaders = lines.filter(_ != headers)
import org.apache.spark.sql.Row
val rows = noheaders.map(_.split(",")).map(a => Row.fromSeq(a))
val movies = spark.createDataFrame(rows, schema)
movies.printSchema


- Creating DataFrame from CSV file
val lines = sc.textFile("/tmp/movies2.dat")
lines: org.apache.spark.rdd.RDD[String] = /tmp/movies2.dat MapPartitionsRDD[3] at textFile at <console>:24
val headers = lines.first
headers: String = id,title,genre

scala> lines.count
res2: Long = 3885

case class Movies(id: Int, title: String, jenre: String)
val noheader = lines.filter(_ != header)
val Mov = noheader.map(_.split(",")).map(r => Movies(r(0).toInt, r(1),r(2)))
scala> val df = Mov.toDF
df: org.apache.spark.sql.DataFrame = [id: int, title: string ... 1 more field]

scala> df.show
+---+--------------------+--------------------+
| id|               title|               jenre|
+---+--------------------+--------------------+
|  1|    Toy Story (1995)|Animation|Childre...|
|  2|      Jumanji (1995)|Adventure|Childre...|
|  3|Grumpier Old Men ...|      Comedy|Romance|
|  4|Waiting to Exhale...|        Comedy|Drama|
|  5|Father of the Bri...|              Comedy|
|  6|         Heat (1995)|Action|Crime|Thri...|
|  7|      Sabrina (1995)|      Comedy|Romance|
|  8| Tom and Huck (1995)|Adventure|Children's|
|  9| Sudden Death (1995)|              Action|
| 10|    GoldenEye (1995)|Action|Adventure|...|
| 11|  American President|          The (1995)|
| 12|Dracula: Dead and...|       Comedy|Horror|
| 13|        Balto (1995)|Animation|Children's|
| 14|        Nixon (1995)|               Drama|
| 15|Cutthroat Island ...|Action|Adventure|...|
| 16|       Casino (1995)|      Drama|Thriller|
| 17|Sense and Sensibi...|       Drama|Romance|
| 18|   Four Rooms (1995)|            Thriller|
| 19|Ace Ventura: When...|              Comedy|
| 20|  Money Train (1995)|              Action|
+---+--------------------+--------------------+
only showing top 20 rows


- Creating DataFrame from CSV files using spark-csv module
val df = spark.read.format("com.databricks.spark.csv").option("header", "true").load("/tmp/movies2.dat")
17/11/27 14:52:22 WARN datasources.DataSource: Error while looking for metadata directory.
df: org.apache.spark.sql.DataFrame = [id: string, title: string ... 1 more field]

scala> df.printSchema
root
 |-- id: string (nullable = true)
 |-- title: string (nullable = true)
 |-- genre: string (nullable = true)

scala> df.show
+---+--------------------+--------------------+
| id|               title|               genre|
+---+--------------------+--------------------+
|  1|    Toy Story (1995)|Animation|Childre...|
|  2|      Jumanji (1995)|Adventure|Childre...|
|  3|Grumpier Old Men ...|      Comedy|Romance|
|  4|Waiting to Exhale...|        Comedy|Drama|
|  5|Father of the Bri...|              Comedy|
|  6|         Heat (1995)|Action|Crime|Thri...|
|  7|      Sabrina (1995)|      Comedy|Romance|
|  8| Tom and Huck (1995)|Adventure|Children's|
|  9| Sudden Death (1995)|              Action|
| 10|    GoldenEye (1995)|Action|Adventure|...|
| 11|  American President|          The (1995)|
| 12|Dracula: Dead and...|       Comedy|Horror|
| 13|        Balto (1995)|Animation|Children's|
| 14|        Nixon (1995)|               Drama|
| 15|Cutthroat Island ...|Action|Adventure|...|
| 16|       Casino (1995)|      Drama|Thriller|
| 17|Sense and Sensibi...|       Drama|Romance|
| 18|   Four Rooms (1995)|            Thriller|
| 19|Ace Ventura: When...|              Comedy|
| 20|  Money Train (1995)|              Action|
+---+--------------------+--------------------+
only showing top 20 rows



- Creating DataFrame from CSV files using spark-csv module
	scala> val df = spark.read.format("com.databricks.spark.csv").option("header", "true").load("/tmp/movies2.dat");
	17/11/29 10:33:29 WARN datasources.DataSource: Error while looking for metadata directory.
	df: org.apache.spark.sql.DataFrame = [id: string, title: string ... 1 more field]

	scala> df.show
	+---+--------------------+--------------------+
	| id|               title|               genre|
	+---+--------------------+--------------------+
	|  1|    Toy Story (1995)|Animation|Childre...|
	|  2|      Jumanji (1995)|Adventure|Childre...|
	|  3|Grumpier Old Men ...|      Comedy|Romance|
	|  4|Waiting to Exhale...|        Comedy|Drama|
	|  5|Father of the Bri...|              Comedy|
	|  6|         Heat (1995)|Action|Crime|Thri...|
	|  7|      Sabrina (1995)|      Comedy|Romance|
	|  8| Tom and Huck (1995)|Adventure|Children's|
	|  9| Sudden Death (1995)|              Action|
	| 10|    GoldenEye (1995)|Action|Adventure|...|
	| 11|  American President|          The (1995)|
	| 12|Dracula: Dead and...|       Comedy|Horror|
	| 13|        Balto (1995)|Animation|Children's|
	| 14|        Nixon (1995)|               Drama|
	| 15|Cutthroat Island ...|Action|Adventure|...|
	| 16|       Casino (1995)|      Drama|Thriller|
	| 17|Sense and Sensibi...|       Drama|Romance|
	| 18|   Four Rooms (1995)|            Thriller|
	| 19|Ace Ventura: When...|              Comedy|
	| 20|  Money Train (1995)|              Action|
	+---+--------------------+--------------------+
	only showing top 20 rows


	scala> df.printSchema
	root
	 |-- id: string (nullable = true)
	 |-- title: string (nullable = true)
	 |-- genre: string (nullable = true)


- Querying DataFrame

	scala> df.select("title", "genre").show	
		
	scala> df.groupBy("genre").count().show(5);
	+--------------------+-----+
	|               genre|count|
	+--------------------+-----+
	| Professore! (Io ...|    1|
	|Adventure|Sci-Fi|...|    2|
	|Comedy|Horror|Thr...|    2|
	|    Adventure|Sci-Fi|    7|
	|Children's|Comedy...|    2|
	+--------------------+-----+	

	scala> df.groupBy("genre").count().sort($"count".desc).show(5);
	+--------------+-----+                                                          
	|         genre|count|
	+--------------+-----+
	|         Drama|  585|
	|        Comedy|  421|
	|  Comedy|Drama|  121|
	|        Horror|  118|
	|Comedy|Romance|  111|
	+--------------+-----+

	scala> import org.apache.spark.sql.functions._
	import org.apache.spark.sql.functions._

	scala> df.groupBy("genre").count().sort(desc("count")).show(5)
	+--------------+-----+                                                          
	|         genre|count|
	+--------------+-----+
	|         Drama|  585|
	|        Comedy|  421|
	|  Comedy|Drama|  121|
	|        Horror|  118|
	|Comedy|Romance|  111|
	+--------------+-----+
	only showing top 5 rows

	scala> df.select("genre").distinct.count
    res8: Long = 572

- Register a DataFrame as a named temporary table to run SQL.

	scala> df.registerTempTable("movies")
	warning: there was one deprecation warning; re-run with -deprecation for details

	scala> val sql = spark.sql("SELECT count(*) AS count FROM movies")
	sql: org.apache.spark.sql.DataFrame = [count: bigint]

	scala> sql.explain
	== Physical Plan ==
	*HashAggregate(keys=[], functions=[count(1)])
	+- Exchange SinglePartition
	   +- *HashAggregate(keys=[], functions=[partial_count(1)])
		  +- *FileScan csv [] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://koreadev/tmp/movies2.dat], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>

	scala> sql.show
	+-----+
	|count|
	+-----+
	| 3883|
	+-----+
	scala> val count = sql.collect()(0).getLong(0)
	count: Long = 3883
	
    scala> df.filter($"genre".like("Act%")).count()
    res16: Long = 394

	scala> df.filter($"genre".like("Act%")).show
	+---+--------------------+--------------------+
	| id|               title|               genre|
	+---+--------------------+--------------------+
	|  6|         Heat (1995)|Action|Crime|Thri...|
	|  9| Sudden Death (1995)|              Action|
	| 10|    GoldenEye (1995)|Action|Adventure|...|
	| 15|Cutthroat Island ...|Action|Adventure|...|
	| 20|  Money Train (1995)|              Action|
	| 21|   Get Shorty (1995)| Action|Comedy|Drama|
	| 42|Dead Presidents (...|  Action|Crime|Drama|
	| 44|Mortal Kombat (1995)|    Action|Adventure|
	| 51|Guardian Angel (1...|Action|Drama|Thri...|
	| 70|From Dusk Till Da...|Action|Comedy|Cri...|
	| 71|    Fair Game (1995)|              Action|
	| 89| Nick of Time (1995)|     Action|Thriller|
	| 95| Broken Arrow (1996)|     Action|Thriller|
	| 98|     Shopping (1994)|     Action|Thriller|
	|110|   Braveheart (1995)|    Action|Drama|War|
	|112|Rumble in the Bro...|Action|Adventure|...|
	|139|       Target (1995)|        Action|Drama|
	|145|     Bad Boys (1995)|              Action|
	|153|Batman Forever (1...|Action|Adventure|...|
	|160|        Congo (1995)|Action|Adventure|...|
	+---+--------------------+--------------------+
	only showing top 20 rows
	
- Example Datasets	
  - Row
    length or size - Row knows the number of elements (columns).
	schema - Row knows the schema
	
	scala> import org.apache.spark.sql.Row
	import org.apache.spark.sql.Row

	scala> val row = Row(1, "hello")
	row: org.apache.spark.sql.Row = [1,hello]

	타입을 지정하지 않고 가져오는 경우는 아래처럼 타입이 아래처럼된다.
	value of type Any
	
	scala> row(1)
	res18: Any = hello

	scala> row.get(1)
	res19: Any = hello
	
    You can query for fields with their proper types using getAs with an index
	scala> val row = Row(1, "hello")
	row: org.apache.spark.sql.Row = [1,hello]

	scala> row.getAs[Int](0)
	res20: Int = 1

	scala> row.getAs[String](1)
	res21: String = hello


	scala> Row(1, "hello")
	res23: org.apache.spark.sql.Row = [1,hello]

	scala> Row.fromSeq(Seq(1, "hello"))
	res24: org.apache.spark.sql.Row = [1,hello]

	scala> Row.fromTuple((0, "hello"))
	res25: org.apache.spark.sql.Row = [0,hello]

	scala> Row.merge(Row(1), Row("hello"))
	res26: org.apache.spark.sql.Row = [1,hello]

- Schema — Structure of Data

	scala>import org.apache.spark.sql.types.StructType
			val schemaUntyped = new StructType()
			.add("a", "int")
			.add("b", "string")
	
	scala> schemaUntyped.printTreeString
	root
	 |-- a: integer (nullable = true)
	 |-- b: string (nullable = true)
		
	scala>import org.apache.spark.sql.types.{IntegerType, StringType}
			val schemaTyped = new StructType()
			.add("a", IntegerType)
			.add("b", StringType)

	scala> schemaTyped.printTreeString
	root
	 |-- a: integer (nullable = true)
	 |-- b: string (nullable = true)
	
	
	
- As of Spark 2.0, you can describe the schema of your strongly-typed datasets using
encoders.
	import org.apache.spark.sql.Encoders
	scala> Encoders.INT.schema.printTreeString
	root
	|-- value: integer (nullable = true)
	scala> Encoders.product[(String, java.sql.Timestamp)].schema.printTreeString
	root
	|-- _1: string (nullable = true)
	|-- _2: timestamp (nullable = true)
	case class Person(id: Long, name: String)
	scala> Encoders.product[Person].schema.printTreeString
	root
	|-- id: long (nullable = false)
	|-- name: string (nullable = true)
	
- Implicit Schema	

val df = Seq((0, s"""hello\tworld"""), (1, "two spaces inside")).toDF("label", "sentence")
scala> df.printSchema
root
|-- label: integer (nullable = false)
|-- sentence: string (nullable = true)
scala> df.schema
res0: org.apache.spark.sql.types.StructType = StructType(StructField(label,IntegerType,false), StructField(sentence,StringType,true))
scala> df.schema("label").dataType
res1: org.apache.spark.sql.types.DataType = IntegerType

- StructType — Data Type for Schema Definition

    schemaUntyped == schemaTyped
	
	import org.apache.spark.sql.types.StructType
	val schemaUntyped = new StructType()
	.add("a", "int")
	.add("b", "string")

	import org.apache.spark.sql.types.{IntegerType, StringType}
	
	val schemaTyped = new StructType()
	.add("a", IntegerType)
	.add("b", StringType)

scala> schemaUntyped == schemaTyped
res0: Boolean = true
- 아래와 같은 방법으로 StructType을 추가할 수 있다.
  You can add a new StructField to your StructType







// json data preparation 
$> vi /tmp/score.json
{"name":"Foo","score": 50},{"name":"Bar","score": 10}

bash-4.1$ hdfs dfs -put -f /tmp/score.json /tmp/

// json을 읽어들여서 파이프 라인을 통해 csv로 전환하는예제
scala> :paste
// Entering paste mode (ctrl-D to finish)

spark.read 
.format("json") 
.load("/tmp/score.json") 
.select("name", "score") 
.where($"score" > 15) 
.write 
.format("csv") 
.save("/tmp/output-csv1") 

// Exiting paste mode, now interpreting.

//result verify
bash-4.1$ hdfs dfs -ls /tmp/output-csv
Found 2 items
-rw-r-----   3 hive supergroup          0 2017-11-24 16:17 /tmp/output-csv/_SUCCESS
-rw-r-----   3 hive supergroup          7 2017-11-24 16:17 /tmp/output-csv/part-00000-acd57c9c-beac-4465-b14f-cdd140de4ecd.csv
bash-4.1$ hdfs dfs -cat /tmp/output-csv/part-00000-acd57c9c-beac-4465-b14f-cdd140de4ecd.csv
Foo,50

// json을 읽어들여서 파이프 라인을 통해 콘솔에 표시하는 예제
load부분에는 경로를 지정할때 directory를 지정해야함.
'basePath' must be a directory


- spark.version 표시
scala> spark.version
res0: String = 2.2.0-SNAPSHOT

- 현재 import된 패키지를 표시해준다.
scala> :imports
1) import spark.implicits._ (59 terms, 38 are implicit)
2) import spark.sql (1 terms)

- 주요 액션
  first,take(3),collect,count

- scala쉘에서 줄바꿈이나 복사 붙여넣기시
  :paste라고 해준다.
  줄바꿈을 할때는 |를 이용한다.

- spark은 하나의 드라이버 프로그램(SparkContext)과 
	다수의 노드에서 여러개의 Executor가 분산실행된다.

- spark shell을 실행하면 sc가 자동생성된다.
	spark을 단독 어플리케이션으로 사용하기 위해서는 
	SparkContext를 직접 초기화 해줘야 한다.
	RDD를 생성하기위해서는 SparkContext가 필요하다.

- pyspark로 작성된 단독프로그램을 실행 하기위해서는
	spark-submit 쉘스크립트를 사용해야한다.
	spark-submit는 파이썬에서 스파크를 연동하는데 필요한 작업들을 포함하고 있다.
	ex)spark-submit my_script.py

- sc 초기화 방법
	setAppName는 application을 구별하기 위한 id와 같은 역할을 하고
	setMaster 부분은 spark을 실행할 cluster를 적어주는 부분이다.
	local은 따로 접속할 필요가 없을을 나타내는 특수한 값이다.
	Spark context available as 'sc' (master = yarn, app id = application_1509580256251_4148).
	
	import org.apache.spark._
	val conf = new SparkConf().setMaster("local").setAppName("basicavgwithkyro");
	val conf = new SparkConf().setMaster("yarn").setAppName("basicavgwithkyro");
	val sc = new SparkContext(conf)
	
	
- 2014~2016 시즌까지 NBA 선수의 성적을 분석한다.  
   - STEP 1
	 csv 파일에는 년도가 없어서 
	 년도를 삽입하는 과정입니다. 해석을 하면 line을 x로 정의했고 
	 filter 함수를 이용해서 x에 ","가 있는 라인만 걸렀습니다.
	
	 for (i <- 2014 to 2016) {
		 println(i)
		 val yearStats = sc.textFile(s"c:\\dataset\\nba\\$i\*")
		 yearStats.filter(x => x.contains(",")).map(x =>  (i,x)).saveAsTextFile(s"c:\\dataset\\nba\\addyear\\$i")
	 }

   - STEP 2 
      아래 예제는 필터는 “FG”문자가 포함된 라인은 제외합니다.
	  *는 ""으로 치환하고 NULL은 계산을 위해 0으로 치환합니다.

	  val stats=sc.textFile("c:\\dataset\\nba\\addyear\\*\\*").repartition(sc.defaultParallelism)
	  stats: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[37] at repartition at <console>:30 
	  //filter out junk rows, clean up data entry errors as well
	  val filteredStats=stats.filter(line => !line.contains("FG%")).filter(line => line.contains(",")).map(line => line.replace("*","").replace(",,",",0,"))
	  filteredStats.cache()
	
   - STEP 3
     process stats and save as map
     val txtStat = Array("FG","FGA","FG%","3P","3PA","3P%","2P","2PA","2P%","eFG%","FT","FTA","FT%","ORB","DRB","TRB","AST","STL","BLK","TOV","PF","PTS")
     val aggStats = processStats(filteredStats,txtStat).collectAsMap

//collect rdd into map and broadcast
val broadcastStats = sc.broadcast(aggStats)	



val wikirdd=rdd.flatMap(
      line=>line.split("\\s") match {
          case Array(project, page, reqCount, contentRepSize) => Some((project,page, reqCount.toInt, contentRepSize))
          case _ => None
          
      }
).filter({case(project,page,reqCount,contentRepSize) => project == "en" }).setName("wiki")
wikirdd.map({
    case (project, page, reqCount, contentRepSize) => (page, reqCount.toInt)  
    
}).map(x=>x.swap).sortByKey(ascending=false).map(x=>x.swap).take(10)


Use SparkSQL DataFrames to find Your Perfect Weather
Inspired by an award winning service http://MyPerfectWeather.eu
Created by Radek Ostrowski https://uk.linkedin.com/in/radekostrowski

import sys.process._
import java.net.URL
import java.io.File
import scala.language.postfixOps
import org.apache.spark.sql.{DataFrame, Row, SQLContext}
import org.apache.spark.sql.functions._
val sqlContext = new SQLContext(sparkContext)
val rawWeatherFile = "/tmp/weather.json"
Download the raw weather prediction data for a set of European cities
It is an outdated and modified weather dataset originally forecasted by IBM The Weather Company

new URL("http://www.fastdata.eu/data/weather.json") #> new File(rawWeatherFile) !!
Read in the json dataset as a DataFrame

val rawWeatherDf = sqlContext.read.format("json").load(rawWeatherFile)
display(rawWeatherDf, maxPoints=1)
rawWeatherDf.count
Let's make more sense out of it and extract some useful weather characteristics

val usefulWeatherDf = rawWeatherDf.select("forecasts.dow", "forecasts.fcst_valid_local",
      "forecasts.num", "forecasts.max_temp", "forecasts.min_temp", "forecasts.day.hi",
      "forecasts.day.wspd", "forecasts.day.wdir_cardinal", "forecasts.day.icon_code",
      "forecasts.day.precip_type", "forecasts.day.pop", "forecasts.day.clds", "forecasts.day.rh",
      "_id", "airport", "country")
display(usefulWeatherDf, maxPoints=2)
case class Weather(dayOfWeek: String, date: String, num: Long,
                     max_temp: Long, min_temp: Long, mean_temp: Long,
                     perceived_temperature: Long, wind_speed: Long, wind_direction_cardinal: String, icon_code: Long,
                     precip_type: String, probability_of_precip: Long, clouds_cover: Long, relative_humidity: Long,
                     city: String, country: String, airport: String) extends java.io.Serializable

import sqlContext.implicits._

val parsedWeatherDf = usefulWeatherDf.flatMap(r => for {
      x <- Range(1, 11)
      y = Weather(r.getList[String](0).get(x), r.getList[String](1).get(x).split("T")(0),
        r.getList[Long](2).get(x), r.getList[Long](3).get(x),
        r.getList[Long](4).get(x), (r.getList[Long](3).get(x) + r.getList[Long](4).get(x)) / 2,
        r.getList[Long](5).get(x), r.getList[Long](6).get(x),
        r.getList[String](7).get(x), r.getList[Long](8).get(x), r.getList[String](9).get(x),
        r.getList[Long](10).get(x), r.getList[Long](11).get(x), r.getList[Long](12).get(x),
        r.getString(13), r.getString(15), r.getString(14))
    } yield y
    ).toDF
display(parsedWeatherDf)

Finally, it looks like something that we could actually use

Now define what exactly is your perfect weather and see matching destinations
Somewhere warm, sunny and dry:
At least 20°C, clouds cover less than 30% and definitely no rain!

display(parsedWeatherDf.where("min_temp >= 20 AND clouds_cover < 30 AND probability_of_precip  = 0"))
// Fly a kite:
Wind speed between 16 and 32 km/h, not too cloudy, not too sunny and zero chance of precipitation

display(parsedWeatherDf.where("wind_speed >= 16 AND wind_speed <= 32 AND clouds_cover > 20 AND clouds_cover < 60 AND probability_of_precip  = 0")
        .select("city", "country").groupBy("city", "country").count().orderBy(desc("count")).withColumnRenamed("count","perfectDaysCount"), maxPoints=10)


Go fishing:
Wind speed less than 16 km/h, high clouds cover
display(parsedWeatherDf.where("wind_speed < 16 AND clouds_cover > 40 AND probability_of_precip < 30")
       .select("city", "country").groupBy("city", "country").count().orderBy(desc("count")).withColumnRenamed("count","perfectDaysCount"), maxPoints=10)
